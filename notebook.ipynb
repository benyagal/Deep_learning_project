{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fd9b10",
   "metadata": {},
   "source": [
    "# ÁSZF Érthetőség Predikciós Modell\n",
    "\n",
    "A projekt célja egy NLP modell létrehozása, amely 1-től 5-ig terjedő skálán megbecsüli jogi szövegrészletek érthetőségét.\n",
    "\n",
    "## Főbb lépések:\n",
    "1. **Adatbetöltés**: Címkézett JSON adatok beolvasása.\n",
    "2. **Jellemzők kinyerése (Feature Engineering)**: Szöveges jellemzők (olvashatósági indexek, jogi terminusok stb.) előállítása.\n",
    "3. **Alapmodell (Baseline)**: Ordinális logisztikus regresszió tanítása a kinyert jellemzőkön.\n",
    "4. **Transformer Modell**: Egyéni CORAL fejjel ellátott transzformer modell (pl. `SZTAKI/HuBERT`) finomhangolása.\n",
    "5. **Értékelés**: Modellek összehasonlítása (MAE, QWK).\n",
    "6. **Hibaanalízis**: A téves predikciók vizsgálata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a04ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback: blank multilingual spaCy model (no entities).\n",
      "Konfiguráció betöltve. Modell: xlm-roberta-base | Eszköz: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Importok és Konfiguráció\n",
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mord import LogisticAT\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 8\n",
    "LR = 2e-5\n",
    "VAL_SIZE = 0.2\n",
    "ANNOTATION_FILE = 'granit_bank_cimkezes.json'\n",
    "TEXT_FILE = 'granit_bank-penzforgalmi_szolgaltatasok_aszf.txt'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# spaCy betöltés robusztus fallback-kel (nincs hálózat)\n",
    "try:\n",
    "    nlp = spacy.load('xx_ent_wiki_sm')\n",
    "except Exception:\n",
    "    print('Fallback: blank multilingual spaCy model (no entities).')\n",
    "    nlp = spacy.blank('xx')\n",
    "    if 'sentencizer' not in nlp.pipe_names:\n",
    "        nlp.add_pipe('sentencizer')\n",
    "\n",
    "print('Konfiguráció betöltve. Modell:', MODEL_NAME, '| Eszköz:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7550f",
   "metadata": {},
   "source": [
    "# 2. Adatbetöltés és Előfeldolgozás\n",
    "\n",
    "A címkézett adatok betöltése a JSON fájlból. A `load_annotation_json` függvény kinyeri a bekezdés szövegét és a hozzá tartozó numerikus címkét."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e295e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betöltött címkék száma: 119\n",
      "Címkék eloszlása:\n",
      "label_int\n",
      "1    14\n",
      "2    12\n",
      "3    28\n",
      "4    43\n",
      "5    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Adatminta:\n",
      "   task_id                                     paragraph_text  label_int  \\\n",
      "0      204  2.1.1. A Pénzforgalmi Keretszerződés alapján a...          2   \n",
      "1      205  2.1.2. A Korlátozottan cselekvőképes kiskorú T...          2   \n",
      "2      206  2.1.3. A Bank által a Bankszámla megnyitásának...          3   \n",
      "3      207  2.1.4. Ha a gazdálkodó szervezet / egyéb szerv...          1   \n",
      "4      208  2.1.5. Ha a gazdálkodó szervezet / egyéb szerv...          3   \n",
      "\n",
      "                 label_text  \n",
      "0         2-Nehezen érthető  \n",
      "1         2-Nehezen érthető  \n",
      "2  3-Többé/kevésbé megértem  \n",
      "3  1-Nagyon nehezen érthető  \n",
      "4  3-Többé/kevésbé megértem  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extract: 100%|██████████| 119/119 [00:00<00:00, 1797.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature oszlopok: ['char_count', 'word_count', 'sentence_count', 'syllable_count', 'flesch_score_hu', 'legal_term_ratio', 'long_word_ratio', 'num_entities'] ...\n",
      "   task_id                                     paragraph_text  label_int  \\\n",
      "0      204  2.1.1. A Pénzforgalmi Keretszerződés alapján a...          2   \n",
      "1      205  2.1.2. A Korlátozottan cselekvőképes kiskorú T...          2   \n",
      "2      206  2.1.3. A Bank által a Bankszámla megnyitásának...          3   \n",
      "\n",
      "                 label_text  char_count  word_count  sentence_count  \\\n",
      "0         2-Nehezen érthető        1158         135               9   \n",
      "1         2-Nehezen érthető        1534         183              11   \n",
      "2  3-Többé/kevésbé megértem        1211         144               5   \n",
      "\n",
      "   syllable_count  flesch_score_hu  legal_term_ratio  long_word_ratio  \\\n",
      "0             391       -53.416667          0.000000         0.155556   \n",
      "1             534       -56.916483          0.005464         0.120219   \n",
      "2             424       -71.497000          0.000000         0.166667   \n",
      "\n",
      "   num_entities  pos_noun_ratio  pos_verb_ratio  pos_adj_ratio  avg_dep_depth  \n",
      "0             0             0.0             0.0            0.0            0.0  \n",
      "1             0             0.0             0.0            0.0            0.0  \n",
      "2             0             0.0             0.0            0.0            0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Adatbetöltés + Haladó Feature Engineering (Label Studio formátum támogatás)\n",
    "\n",
    "def load_annotation_json(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölti a címkézési adatokat egy JSON fájlból és DataFrame-mé alakítja.\n",
    "    Elvárt felépítés (Label Studio export): lista a task objektumokkal.\n",
    "    - task['data']['text'] tartalmazza a bekezdés szövegét\n",
    "    - task['annotations'][0]['result'][0]['value']['choices'][0] tartalmazza a label szöveget (pl. '3 - Közepes')\n",
    "    A labelt az első számjegy alapján nyeri ki.\n",
    "    Ha a fájl üres vagy hibás, üres DataFrame-et ad vissza.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"Figyelem: A fájl nem található: {path}\")\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    raw = p.read_text(encoding='utf-8').strip()\n",
    "    if not raw:\n",
    "        print(\"Figyelem: A JSON fájl üres.\")\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    if not raw.startswith('['):  # ha véletlenül nem listaként mentették\n",
    "        raw = f'[{raw}]'\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print('JSON decode hiba:', e)\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    rows = []\n",
    "    for task in data:\n",
    "        text = task.get('data', {}).get('text', '').strip()\n",
    "        ann_list = task.get('annotations', [])\n",
    "        if not ann_list:\n",
    "            continue\n",
    "        ann = ann_list[0]\n",
    "        result = ann.get('result', [])\n",
    "        if not result:\n",
    "            continue\n",
    "        choice = result[0].get('value', {}).get('choices', [None])[0]\n",
    "        if not choice:\n",
    "            continue\n",
    "        m = re.match(r'(\\d)', str(choice))\n",
    "        if not m:\n",
    "            continue\n",
    "        label_int = int(m.group(1))\n",
    "        rows.append({\n",
    "            'task_id': task.get('id'),\n",
    "            'paragraph_text': text,\n",
    "            'label_int': label_int,\n",
    "            'label_text': choice\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Jogi terminus lista (bővíthető)\n",
    "LEGAL_TERMS = [\"szerződés\",\"feltétel\",\"jog\",\"kötelezettség\",\"felelősség\",\"kártérítés\",\"hatály\",\"rendelkezés\",\"törvény\",\"rendelet\",\"bíróság\",\"per\",\"felmondás\",\"biztosítás\",\"ügyfél\"]\n",
    "VOWELS = \"aáeéiíoóöőuúüű\"\n",
    "\n",
    "def count_syllables_hu(word):\n",
    "    count = 0; in_grp = False\n",
    "    for ch in word.lower():\n",
    "        if ch in VOWELS:\n",
    "            if not in_grp:\n",
    "                count += 1; in_grp = True\n",
    "        else:\n",
    "            in_grp = False\n",
    "    return max(1, count)\n",
    "\n",
    "def extract_features(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n",
    "    syllable_count = sum(count_syllables_hu(w) for w in words) if words else 0\n",
    "    avg_words_per_sentence = word_count / sentence_count if sentence_count else 0\n",
    "    avg_syllables_per_word = syllable_count / word_count if word_count else 0\n",
    "    flesch_score_hu = 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables_per_word if word_count else 0\n",
    "    legal_term_ratio = sum(1 for w in words if w in LEGAL_TERMS) / word_count if word_count else 0\n",
    "    long_word_ratio = sum(1 for w in words if len(w) > 12) / word_count if word_count else 0\n",
    "    # spaCy elemzés (blank modell esetén entitás 0 marad)\n",
    "    doc = nlp(text)\n",
    "    num_entities = len(doc.ents) if doc.ents else 0\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS) if hasattr(spacy.attrs,'POS') else {}\n",
    "    num_nouns = pos_counts.get(spacy.symbols.NOUN, 0) if hasattr(spacy.symbols,'NOUN') else 0\n",
    "    num_verbs = pos_counts.get(spacy.symbols.VERB, 0) if hasattr(spacy.symbols,'VERB') else 0\n",
    "    num_adjs  = pos_counts.get(spacy.symbols.ADJ, 0) if hasattr(spacy.symbols,'ADJ') else 0\n",
    "    pos_noun_ratio = num_nouns / word_count if word_count else 0\n",
    "    pos_verb_ratio = num_verbs / word_count if word_count else 0\n",
    "    pos_adj_ratio  = num_adjs  / word_count if word_count else 0\n",
    "    depths = []\n",
    "    for token in doc:\n",
    "        d=0; cur=token\n",
    "        while cur.head != cur and d < 100:\n",
    "            d += 1; cur = cur.head\n",
    "        depths.append(d)\n",
    "    avg_dep_depth = float(np.mean(depths)) if depths else 0\n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'syllable_count': syllable_count,\n",
    "        'flesch_score_hu': flesch_score_hu,\n",
    "        'legal_term_ratio': legal_term_ratio,\n",
    "        'long_word_ratio': long_word_ratio,\n",
    "        'num_entities': num_entities,\n",
    "        'pos_noun_ratio': pos_noun_ratio,\n",
    "        'pos_verb_ratio': pos_verb_ratio,\n",
    "        'pos_adj_ratio': pos_adj_ratio,\n",
    "        'avg_dep_depth': avg_dep_depth\n",
    "    }\n",
    "\n",
    "# Annotációk betöltése az új függvénnyel\n",
    "df_labels = load_annotation_json(ANNOTATION_FILE)\n",
    "if not df_labels.empty:\n",
    "    print(f\"Betöltött címkék száma: {len(df_labels)}\")\n",
    "    print(\"Címkék eloszlása:\")\n",
    "    print(df_labels['label_int'].value_counts().sort_index())\n",
    "    print(\"\\nAdatminta:\")\n",
    "    print(df_labels.head())\n",
    "else:\n",
    "    print(\"Nem sikerült adatokat betölteni. A további feature / modell lépések kihagyva amíg nincs adat.\")\n",
    "\n",
    "# Feature kinyerés csak ha van adat\n",
    "if not df_labels.empty:\n",
    "    feature_rows = [extract_features(t) for t in tqdm(df_labels['paragraph_text'], desc='Feature extract')] \n",
    "    df_feat = pd.DataFrame(feature_rows)\n",
    "    df_processed = pd.concat([df_labels.reset_index(drop=True), df_feat], axis=1)\n",
    "    print('Feature oszlopok:', list(df_feat.columns)[:8], '...')\n",
    "    print(df_processed.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb40083",
   "metadata": {},
   "source": [
    "# 4. Alapmodell (Baseline) - Ordinális Regresszió\n",
    "\n",
    "Egy egyszerű, de hatékony alapmodellt tanítunk a kinyert jellemzőkön a `mord` csomag segítségével."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7c616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LogisticAT MAE: 0.9583 (cél < 0.9167)\n"
     ]
    }
   ],
   "source": [
    "# 5. Baseline Ordinal Regression (LogisticAT) haladó feature-ökkel\n",
    "if 'df_processed' in globals() and not df_processed.empty:\n",
    "    baseline_feature_cols = [\n",
    "        'char_count','word_count','sentence_count','syllable_count','flesch_score_hu',\n",
    "        'legal_term_ratio','long_word_ratio','num_entities','pos_noun_ratio','pos_verb_ratio',\n",
    "        'pos_adj_ratio','avg_dep_depth'\n",
    "    ]\n",
    "    missing = [c for c in baseline_feature_cols if c not in df_processed.columns]\n",
    "    if missing:\n",
    "        print('Hiányzó feature oszlopok, baseline kihagyva:', missing)\n",
    "    else:\n",
    "        Xb = df_processed[baseline_feature_cols].values\n",
    "        yb = df_processed['label_int'].values\n",
    "        X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(Xb, yb, test_size=VAL_SIZE, random_state=SEED, stratify=yb)\n",
    "        scaler_b = StandardScaler()\n",
    "        X_train_b = scaler_b.fit_transform(X_train_b)\n",
    "        X_val_b = scaler_b.transform(X_val_b)\n",
    "        baseline_model = LogisticAT(alpha=0.5)\n",
    "        baseline_model.fit(X_train_b, y_train_b)\n",
    "        preds_b = baseline_model.predict(X_val_b)\n",
    "        mae_b = mean_absolute_error(y_val_b, preds_b)\n",
    "        print(f'Baseline LogisticAT MAE: {mae_b:.4f}')\n",
    "else:\n",
    "    print('Baseline nem futtatható: df_processed hiányzik vagy üres.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f91436",
   "metadata": {},
   "source": [
    "# 5. Transformer Modell (CORAL)\n",
    "\n",
    "Egy neurális háló alapú modellt definiálunk, amely egy előtanított transzformert (pl. huBERT) használ, és egy CORAL (Cumulative Ordinal Ranking and Regression) kimeneti réteggel egészíti ki az ordinális klasszifikációhoz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e7fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer + CORAL modell betöltve. Extra feature dim: 12\n"
     ]
    }
   ],
   "source": [
    "# --- CORAL Modell definíciója (feature integráció + expected value támogatás) ---\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "class CoralHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, extra_feat_dim=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.use_extra = extra_feat_dim > 0\n",
    "        in_dim = hidden_size + extra_feat_dim if self.use_extra else hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(in_dim, num_classes - 1)\n",
    "    def forward(self, cls_hidden, extra_feats=None):\n",
    "        if self.use_extra and extra_feats is not None:\n",
    "            x = torch.cat([cls_hidden, extra_feats], dim=1)\n",
    "        else:\n",
    "            x = cls_hidden\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "def coral_probs_to_label_argmax(probs):\n",
    "    batch_size = probs.size(0)\n",
    "    ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "    zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "    p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "    p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "    return torch.argmax(p_exact, dim=1) + 1\n",
    "\n",
    "def coral_probs_to_label_expected(probs):\n",
    "    batch_size = probs.size(0)\n",
    "    ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "    zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "    p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "    p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "    labels = torch.arange(1, probs.size(1)+2, device=probs.device).float()  # 1..K\n",
    "    exp_val = torch.sum(p_exact * labels, dim=1)\n",
    "    return torch.clamp(torch.round(exp_val), 1, probs.size(1)+1).long()\n",
    "\n",
    "class CoralModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=5, extra_feat_dim=0):\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.base.config.hidden_size\n",
    "        self.head = CoralHead(hidden_size, num_classes, extra_feat_dim=extra_feat_dim, dropout=0.3)\n",
    "    def forward(self, input_ids, attention_mask, extra_feats=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        probs = self.head(cls_output, extra_feats)\n",
    "        return probs\n",
    "\n",
    "def build_pos_weights(labels, num_classes):\n",
    "    # labels 1..K; for each threshold k (1..K-1) compute pos = labels>k\n",
    "    weights = []\n",
    "    labels_t = torch.tensor(labels)\n",
    "    for k in range(1, num_classes):\n",
    "        pos = (labels_t > k).sum().item()\n",
    "        neg = (labels_t <= k).sum().item()\n",
    "        if pos == 0: pos = 1\n",
    "        w_pos = neg / pos  # emphasize minority positives\n",
    "        weights.append(w_pos)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "def coral_loss_weighted(probs, labels, pos_weights):\n",
    "    # probs shape (B, K-1); labels 1..K\n",
    "    K = probs.size(1) + 1\n",
    "    targets = []\n",
    "    for k in range(1, K):\n",
    "        target_k = (labels > k).float().unsqueeze(1)\n",
    "        targets.append(target_k)\n",
    "    target_tensor = torch.cat(targets, dim=1)  # (B, K-1)\n",
    "    # Weighted BCE manually\n",
    "    # pos_weight for positive target; negative weight = 1\n",
    "    pw = pos_weights.to(probs.device).unsqueeze(0)  # (1,K-1)\n",
    "    loss_pos = -pw * target_tensor * torch.log(probs + 1e-8)\n",
    "    loss_neg = -(1 - target_tensor) * torch.log(1 - probs + 1e-8)\n",
    "    loss = (loss_pos + loss_neg).mean()\n",
    "    return loss\n",
    "\n",
    "# Tokenizer + modell példányosítás extra feature dim alapján\n",
    "extra_dim = len(['char_count','word_count','sentence_count','long_word_ratio','syllable_count','flesch_score_hu','legal_term_ratio','num_entities','pos_noun_ratio','pos_verb_ratio','pos_adj_ratio','avg_dep_depth'])\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "coral_model = CoralModel(MODEL_NAME, num_classes=NUM_CLASSES, extra_feat_dim=extra_dim)\n",
    "print('Transformer + CORAL modell betöltve. Extra feature dim:', extra_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6927ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Train size=95 Val size=24\n",
      "Fold 1 Epoch 1: TrainLoss=0.5994 ValLoss=0.5349 ValMAE=1.0417\n",
      "Fold 1 Epoch 1: TrainLoss=0.5994 ValLoss=0.5349 ValMAE=1.0417\n",
      "Fold 1 Epoch 2: TrainLoss=0.5477 ValLoss=0.5370 ValMAE=0.9583\n",
      "Fold 1 Epoch 2: TrainLoss=0.5477 ValLoss=0.5370 ValMAE=0.9583\n",
      "Fold 1 Epoch 3: TrainLoss=0.5174 ValLoss=0.5291 ValMAE=1.0000\n",
      "Fold 1 Epoch 3: TrainLoss=0.5174 ValLoss=0.5291 ValMAE=1.0000\n",
      "Fold 1 Epoch 4: TrainLoss=0.5233 ValLoss=0.5330 ValMAE=1.0417\n",
      "Early stopping fold 1\n",
      "Fold 1 best MAE: 0.9583\n",
      "\n",
      "[Fold 2/5] Train size=95 Val size=24\n",
      "Fold 1 Epoch 4: TrainLoss=0.5233 ValLoss=0.5330 ValMAE=1.0417\n",
      "Early stopping fold 1\n",
      "Fold 1 best MAE: 0.9583\n",
      "\n",
      "[Fold 2/5] Train size=95 Val size=24\n",
      "Fold 2 Epoch 1: TrainLoss=0.6358 ValLoss=0.5700 ValMAE=1.5417\n",
      "Fold 2 Epoch 1: TrainLoss=0.6358 ValLoss=0.5700 ValMAE=1.5417\n",
      "Fold 2 Epoch 2: TrainLoss=0.5311 ValLoss=0.5091 ValMAE=1.0000\n",
      "Fold 2 Epoch 2: TrainLoss=0.5311 ValLoss=0.5091 ValMAE=1.0000\n",
      "Fold 2 Epoch 3: TrainLoss=0.5441 ValLoss=0.5116 ValMAE=0.9583\n",
      "Fold 2 Epoch 3: TrainLoss=0.5441 ValLoss=0.5116 ValMAE=0.9583\n",
      "Fold 2 Epoch 4: TrainLoss=0.5053 ValLoss=0.4967 ValMAE=0.9167\n",
      "Fold 2 Epoch 4: TrainLoss=0.5053 ValLoss=0.4967 ValMAE=0.9167\n",
      "Fold 2 Epoch 5: TrainLoss=0.5018 ValLoss=0.5028 ValMAE=0.9583\n",
      "Fold 2 Epoch 5: TrainLoss=0.5018 ValLoss=0.5028 ValMAE=0.9583\n",
      "Fold 2 Epoch 6: TrainLoss=0.4792 ValLoss=0.4963 ValMAE=0.9583\n",
      "Early stopping fold 2\n",
      "Fold 2 best MAE: 0.9167\n",
      "\n",
      "[Fold 3/5] Train size=95 Val size=24\n",
      "Fold 2 Epoch 6: TrainLoss=0.4792 ValLoss=0.4963 ValMAE=0.9583\n",
      "Early stopping fold 2\n",
      "Fold 2 best MAE: 0.9167\n",
      "\n",
      "[Fold 3/5] Train size=95 Val size=24\n",
      "Fold 3 Epoch 1: TrainLoss=0.5906 ValLoss=0.5688 ValMAE=1.5833\n",
      "Fold 3 Epoch 1: TrainLoss=0.5906 ValLoss=0.5688 ValMAE=1.5833\n",
      "Fold 3 Epoch 2: TrainLoss=0.5576 ValLoss=0.5015 ValMAE=0.9583\n",
      "Fold 3 Epoch 2: TrainLoss=0.5576 ValLoss=0.5015 ValMAE=0.9583\n",
      "Fold 3 Epoch 3: TrainLoss=0.5048 ValLoss=0.5451 ValMAE=1.0000\n",
      "Fold 3 Epoch 3: TrainLoss=0.5048 ValLoss=0.5451 ValMAE=1.0000\n",
      "Fold 3 Epoch 4: TrainLoss=0.5310 ValLoss=0.5647 ValMAE=1.0000\n",
      "Early stopping fold 3\n",
      "Fold 3 best MAE: 0.9583\n",
      "\n",
      "[Fold 4/5] Train size=95 Val size=24\n",
      "Fold 3 Epoch 4: TrainLoss=0.5310 ValLoss=0.5647 ValMAE=1.0000\n",
      "Early stopping fold 3\n",
      "Fold 3 best MAE: 0.9583\n",
      "\n",
      "[Fold 4/5] Train size=95 Val size=24\n",
      "Fold 4 Epoch 1: TrainLoss=0.6489 ValLoss=0.6144 ValMAE=1.7500\n",
      "Fold 4 Epoch 1: TrainLoss=0.6489 ValLoss=0.6144 ValMAE=1.7500\n",
      "Fold 4 Epoch 2: TrainLoss=0.5532 ValLoss=0.5155 ValMAE=0.9583\n",
      "Fold 4 Epoch 2: TrainLoss=0.5532 ValLoss=0.5155 ValMAE=0.9583\n",
      "Fold 4 Epoch 3: TrainLoss=0.5179 ValLoss=0.5164 ValMAE=1.0000\n",
      "Fold 4 Epoch 3: TrainLoss=0.5179 ValLoss=0.5164 ValMAE=1.0000\n",
      "Fold 4 Epoch 4: TrainLoss=0.5457 ValLoss=0.5116 ValMAE=1.0000\n",
      "Early stopping fold 4\n",
      "Fold 4 best MAE: 0.9583\n",
      "\n",
      "[Fold 5/5] Train size=96 Val size=23\n",
      "Fold 4 Epoch 4: TrainLoss=0.5457 ValLoss=0.5116 ValMAE=1.0000\n",
      "Early stopping fold 4\n",
      "Fold 4 best MAE: 0.9583\n",
      "\n",
      "[Fold 5/5] Train size=96 Val size=23\n",
      "Fold 5 Epoch 1: TrainLoss=0.6001 ValLoss=0.4934 ValMAE=0.9130\n",
      "Fold 5 Epoch 1: TrainLoss=0.6001 ValLoss=0.4934 ValMAE=0.9130\n",
      "Fold 5 Epoch 2: TrainLoss=0.5580 ValLoss=0.5492 ValMAE=2.0435\n",
      "Fold 5 Epoch 2: TrainLoss=0.5580 ValLoss=0.5492 ValMAE=2.0435\n",
      "Fold 5 Epoch 3: TrainLoss=0.5547 ValLoss=0.4883 ValMAE=0.8696\n",
      "Fold 5 Epoch 3: TrainLoss=0.5547 ValLoss=0.4883 ValMAE=0.8696\n",
      "Fold 5 Epoch 4: TrainLoss=0.5135 ValLoss=0.4695 ValMAE=0.8261\n",
      "Fold 5 Epoch 4: TrainLoss=0.5135 ValLoss=0.4695 ValMAE=0.8261\n",
      "Fold 5 Epoch 5: TrainLoss=0.5125 ValLoss=0.4627 ValMAE=0.7826\n",
      "Fold 5 Epoch 5: TrainLoss=0.5125 ValLoss=0.4627 ValMAE=0.7826\n",
      "Fold 5 Epoch 6: TrainLoss=0.5079 ValLoss=0.4561 ValMAE=0.8261\n",
      "Fold 5 best MAE: 0.7826\n",
      "\n",
      "K-Fold MAE-k: [0.9583333333333334, 0.9166666666666666, 0.9583333333333334, 0.9583333333333334, 0.782608695652174]\n",
      "Átlagos MAE (fold átlag): 0.9148550724637682\n",
      "Fold 5 Epoch 6: TrainLoss=0.5079 ValLoss=0.4561 ValMAE=0.8261\n",
      "Fold 5 best MAE: 0.7826\n",
      "\n",
      "K-Fold MAE-k: [0.9583333333333334, 0.9166666666666666, 0.9583333333333334, 0.9583333333333334, 0.782608695652174]\n",
      "Átlagos MAE (fold átlag): 0.9148550724637682\n"
     ]
    }
   ],
   "source": [
    "# 7. 5-Fold Stratified Cross-Validation Ensemble\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "KFOLDS = 5\n",
    "EPOCHS_KF = 6\n",
    "PATIENCE_KF = 2\n",
    "LEARNING_RATE_KF = 2e-5\n",
    "\n",
    "if 'df_processed' in globals() and not df_processed.empty:\n",
    "    skf = StratifiedKFold(n_splits=KFOLDS, shuffle=True, random_state=SEED)\n",
    "    labels_all = df_processed['label_int'].values\n",
    "    fold_results = []\n",
    "    ensemble_exact_probs = []  # store per-fold val exact class prob for ensemble\n",
    "\n",
    "    # Helper: simple CORAL loss\n",
    "    def coral_loss_simple(probs, labels):\n",
    "        K = probs.size(1) + 1\n",
    "        targets = []\n",
    "        for k in range(1, K):\n",
    "            target_k = (labels > k).float().unsqueeze(1)\n",
    "            targets.append(target_k)\n",
    "        target_tensor = torch.cat(targets, dim=1)\n",
    "        return nn.BCELoss()(probs, target_tensor)\n",
    "\n",
    "    # Function to get exact class probabilities from cumulative probs\n",
    "    def cumulative_to_exact(probs):\n",
    "        batch_size = probs.size(0)\n",
    "        ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "        zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "        p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "        p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "        return p_exact  # shape (B, K)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_processed, labels_all), start=1):\n",
    "        print(f'\\n[Fold {fold}/{KFOLDS}] Train size={len(train_idx)} Val size={len(val_idx)}')\n",
    "        df_train_f = df_processed.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val_f = df_processed.iloc[val_idx].reset_index(drop=True)\n",
    "        # Stats per fold\n",
    "        feature_stats_f = {c: (df_train_f[c].mean(), df_train_f[c].std() if df_train_f[c].std() > 0 else 1.0) for c in advanced_feature_cols}\n",
    "\n",
    "        train_loader_f = create_data_loader(df_train_f, tokenizer, MAX_LEN, BATCH_SIZE, advanced_feature_cols, feature_stats_f)\n",
    "        val_loader_f = create_data_loader(df_val_f, tokenizer, MAX_LEN, BATCH_SIZE, advanced_feature_cols, feature_stats_f)\n",
    "\n",
    "        # Fresh model per fold\n",
    "        model_f = CoralModel(MODEL_NAME, num_classes=NUM_CLASSES, extra_feat_dim=len(advanced_feature_cols)).to(DEVICE)\n",
    "        optimizer_f = torch.optim.AdamW(model_f.parameters(), lr=LEARNING_RATE_KF)\n",
    "        total_steps_f = len(train_loader_f) * EPOCHS_KF\n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "        scheduler_f = get_linear_schedule_with_warmup(optimizer_f, num_warmup_steps=0, num_training_steps=total_steps_f)\n",
    "\n",
    "        best_mae_f = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(EPOCHS_KF):\n",
    "            model_f.train(); train_losses=[]\n",
    "            for batch in train_loader_f:\n",
    "                ids = batch['input_ids'].to(DEVICE)\n",
    "                mask = batch['attention_mask'].to(DEVICE)\n",
    "                feats = batch['features'].to(DEVICE) if batch['features'] is not None else None\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                probs = model_f(ids, mask, extra_feats=feats)\n",
    "                loss = coral_loss_simple(probs, labels)\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward(); nn.utils.clip_grad_norm_(model_f.parameters(),1.0)\n",
    "                optimizer_f.step(); scheduler_f.step(); optimizer_f.zero_grad()\n",
    "            # Validation\n",
    "            model_f.eval(); all_lab=[]; all_pred=[]; val_losses=[]; val_exact_prob_collect=[]\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader_f:\n",
    "                    ids = batch['input_ids'].to(DEVICE)\n",
    "                    mask = batch['attention_mask'].to(DEVICE)\n",
    "                    feats = batch['features'].to(DEVICE) if batch['features'] is not None else None\n",
    "                    labels = batch['labels'].to(DEVICE)\n",
    "                    probs = model_f(ids, mask, extra_feats=feats)\n",
    "                    loss = coral_loss_simple(probs, labels)\n",
    "                    val_losses.append(loss.item())\n",
    "                    p_exact = cumulative_to_exact(probs)\n",
    "                    preds = torch.argmax(p_exact, dim=1) + 1\n",
    "                    all_lab.extend(labels.cpu().numpy())\n",
    "                    all_pred.extend(preds.cpu().numpy())\n",
    "                    val_exact_prob_collect.append(p_exact.cpu())\n",
    "            mae_f = mean_absolute_error(all_lab, all_pred)\n",
    "            print(f'Fold {fold} Epoch {epoch+1}: TrainLoss={np.mean(train_losses):.4f} ValLoss={np.mean(val_losses):.4f} ValMAE={mae_f:.4f}')\n",
    "            if mae_f < best_mae_f:\n",
    "                best_mae_f = mae_f\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model_f.state_dict(), f'coral_fold{fold}.bin')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= PATIENCE_KF:\n",
    "                    print('Early stopping fold', fold)\n",
    "                    break\n",
    "        print(f'Fold {fold} best MAE: {best_mae_f:.4f}')\n",
    "        fold_results.append(best_mae_f)\n",
    "        # Store ensemble probabilities from best epoch (we used last collected p_exact list)\n",
    "        ensemble_exact_probs.append(torch.cat(val_exact_prob_collect, dim=0))\n",
    "    mean_mae = np.mean(fold_results)\n",
    "    print('\\nK-Fold MAE-k:', fold_results)\n",
    "    print('Átlagos MAE (fold átlag):', mean_mae)\n",
    "    # Ensemble across folds on their own validation splits is not directly comparable; we can just report mean.\n",
    "    # (Optionally could retrain on full data with averaged thresholds.)\n",
    "else:\n",
    "    print('Nincs adat a K-fold futtatáshoz.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c51c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
