{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fd9b10",
   "metadata": {},
   "source": [
    "# ÁSZF Érthetőség Predikciós Modell\n",
    "\n",
    "A projekt célja egy NLP modell létrehozása, amely 1-től 5-ig terjedő skálán megbecsüli jogi szövegrészletek érthetőségét.\n",
    "\n",
    "## Főbb lépések:\n",
    "1. **Adatbetöltés**: Címkézett JSON adatok beolvasása.\n",
    "2. **Jellemzők kinyerése (Feature Engineering)**: Szöveges jellemzők (olvashatósági indexek, jogi terminusok stb.) előállítása.\n",
    "3. **Alapmodell (Baseline)**: Ordinális logisztikus regresszió tanítása a kinyert jellemzőkön.\n",
    "4. **Transformer Modell**: Egyéni CORAL fejjel ellátott transzformer modell (pl. `SZTAKI/HuBERT`) finomhangolása.\n",
    "5. **Értékelés**: Modellek összehasonlítása (MAE, QWK).\n",
    "6. **Hibaanalízis**: A téves predikciók vizsgálata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32684fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'hu_core_news_md' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Ha nem engedi akkor más kell\n",
    "!git clone https://huggingface.co/huspacy/hu_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b80fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lokális magyar modell található: c:\\Users\\galb1\\Documents\\melytanulas\\hu_core_news_md\n",
      "A modell közvetlenül a mappából fog betöltődni a következő cellában.\n"
     ]
    }
   ],
   "source": [
    "# Magyar spaCy modell (HuSpaCy) telepítése lokális forrásból\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# A lokális hu_core_news_md mappa elérési útja\n",
    "local_model_path = Path(\"hu_core_news_md\")\n",
    "\n",
    "if local_model_path.exists():\n",
    "    print(f\"✓ Lokális magyar modell található: {local_model_path.absolute()}\")\n",
    "    print(\"A modell közvetlenül a mappából fog betöltődni a következő cellában.\")\n",
    "else:\n",
    "    print(f\"⚠ Figyelem: A '{local_model_path}' mappa nem található!\")\n",
    "    print(\"Bizonyosodj meg róla, hogy a klónozott repository ebben a könyvtárban van.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2922c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\galb1\\documents\\melytanulas\\hu_core_news_md\\hu_core_news_md-3.8.1-py3-none-any.whl\n",
      "Requirement already satisfied: spacy<3.9.0,>=3.8.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from hu-core-news-md==3.8.1) (3.8.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.0.11)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (8.3.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.3.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (21.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (1.3.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from packaging>=20.0->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\galb1\\documents\\melytanulas\\.conda\\lib\\site-packages (from jinja2->spacy<3.9.0,>=3.8.0->hu-core-news-md==3.8.1) (3.0.3)\n",
      "Installing collected packages: hu-core-news-md\n",
      "Successfully installed hu-core-news-md-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"hu_core_news_md\\hu_core_news_md-3.8.1-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a04ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Magyar spaCy modell betöltve lokális forrásból: hu_core_news_md\n",
      "Konfiguráció betöltve. Modell: xlm-roberta-base | Eszköz: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Importok és Konfiguráció\n",
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mord import LogisticAT\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 8\n",
    "LR = 2e-5\n",
    "VAL_SIZE = 0.2\n",
    "ANNOTATION_FILE = 'granit_bank_cimkezes.json'\n",
    "TEXT_FILE = 'granit_bank-penzforgalmi_szolgaltatasok_aszf.txt'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Magyar spaCy modell betöltése (lokális hu_core_news_md mappából)\n",
    "local_model_path = \"hu_core_news_md\"\n",
    "try:\n",
    "    nlp = spacy.load(local_model_path)\n",
    "    print(f'✓ Magyar spaCy modell betöltve lokális forrásból: {local_model_path}')\n",
    "except Exception as e:\n",
    "    print(f'⚠ Lokális modell betöltése sikertelen ({e}).')\n",
    "    print('  Fallback: blank magyar modell (kevésbé pontos POS/NER)')\n",
    "    nlp = spacy.blank('hu')\n",
    "    if 'sentencizer' not in nlp.pipe_names:\n",
    "        nlp.add_pipe('sentencizer')\n",
    "\n",
    "print('Konfiguráció betöltve. Modell:', MODEL_NAME, '| Eszköz:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7550f",
   "metadata": {},
   "source": [
    "# 2. Adatbetöltés és Előfeldolgozás\n",
    "\n",
    "A címkézett adatok betöltése a JSON fájlból. A `load_annotation_json` függvény kinyeri a bekezdés szövegét és a hozzá tartozó numerikus címkét."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e295e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betöltött címkék száma: 119\n",
      "Címkék eloszlása:\n",
      "label_int\n",
      "1    14\n",
      "2    12\n",
      "3    28\n",
      "4    43\n",
      "5    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Adatminta:\n",
      "   task_id                                     paragraph_text  label_int  \\\n",
      "0      204  2.1.1. A Pénzforgalmi Keretszerződés alapján a...          2   \n",
      "1      205  2.1.2. A Korlátozottan cselekvőképes kiskorú T...          2   \n",
      "2      206  2.1.3. A Bank által a Bankszámla megnyitásának...          3   \n",
      "3      207  2.1.4. Ha a gazdálkodó szervezet / egyéb szerv...          1   \n",
      "4      208  2.1.5. Ha a gazdálkodó szervezet / egyéb szerv...          3   \n",
      "\n",
      "                 label_text  \n",
      "0         2-Nehezen érthető  \n",
      "1         2-Nehezen érthető  \n",
      "2  3-Többé/kevésbé megértem  \n",
      "3  1-Nagyon nehezen érthető  \n",
      "4  3-Többé/kevésbé megértem  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extract: 100%|██████████| 119/119 [00:05<00:00, 21.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature oszlopok: ['char_count', 'word_count', 'sentence_count', 'syllable_count', 'avg_word_length', 'flesch_score_hu', 'gunning_fog', 'smog_index'] ...\n",
      "   task_id                                     paragraph_text  label_int  \\\n",
      "0      204  2.1.1. A Pénzforgalmi Keretszerződés alapján a...          2   \n",
      "1      205  2.1.2. A Korlátozottan cselekvőképes kiskorú T...          2   \n",
      "2      206  2.1.3. A Bank által a Bankszámla megnyitásának...          3   \n",
      "\n",
      "                 label_text  char_count  word_count  sentence_count  \\\n",
      "0         2-Nehezen érthető        1158         135               9   \n",
      "1         2-Nehezen érthető        1534         183              11   \n",
      "2  3-Többé/kevésbé megértem        1211         144               5   \n",
      "\n",
      "   syllable_count  avg_word_length  flesch_score_hu  ...  comma_ratio  \\\n",
      "0             391         7.400000       -53.416667  ...     0.051852   \n",
      "1             534         7.262295       -56.916483  ...     0.043716   \n",
      "2             424         7.263889       -71.497000  ...     0.076389   \n",
      "\n",
      "   parenthesis_ratio  uppercase_ratio  num_entities  pos_noun_ratio  \\\n",
      "0           0.222222         0.029630            19        0.214815   \n",
      "1           0.181818         0.043716            16        0.273224   \n",
      "2           0.400000         0.013889            13        0.291667   \n",
      "\n",
      "   pos_verb_ratio  pos_adj_ratio  pos_adv_ratio  avg_dep_depth  max_dep_depth  \n",
      "0        0.074074       0.214815       0.044444       3.085526            8.0  \n",
      "1        0.071038       0.218579       0.049180       2.676617            6.0  \n",
      "2        0.041667       0.215278       0.034722       3.443750            8.0  \n",
      "\n",
      "[3 rows x 27 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Adatbetöltés + Haladó Feature Engineering (Label Studio formátum támogatás)\n",
    "\n",
    "def load_annotation_json(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölti a címkézési adatokat egy JSON fájlból és DataFrame-mé alakítja.\n",
    "    Elvárt felépítés (Label Studio export): lista a task objektumokkal.\n",
    "    - task['data']['text'] tartalmazza a bekezdés szövegét\n",
    "    - task['annotations'][0]['result'][0]['value']['choices'][0] tartalmazza a label szöveget (pl. '3 - Közepes')\n",
    "    A labelt az első számjegy alapján nyeri ki.\n",
    "    Ha a fájl üres vagy hibás, üres DataFrame-et ad vissza.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"Figyelem: A fájl nem található: {path}\")\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    raw = p.read_text(encoding='utf-8').strip()\n",
    "    if not raw:\n",
    "        print(\"Figyelem: A JSON fájl üres.\")\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    if not raw.startswith('['):  # ha véletlenül nem listaként mentették\n",
    "        raw = f'[{raw}]'\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print('JSON decode hiba:', e)\n",
    "        return pd.DataFrame(columns=['task_id','paragraph_text','label_int','label_text'])\n",
    "    rows = []\n",
    "    for task in data:\n",
    "        text = task.get('data', {}).get('text', '').strip()\n",
    "        ann_list = task.get('annotations', [])\n",
    "        if not ann_list:\n",
    "            continue\n",
    "        ann = ann_list[0]\n",
    "        result = ann.get('result', [])\n",
    "        if not result:\n",
    "            continue\n",
    "        choice = result[0].get('value', {}).get('choices', [None])[0]\n",
    "        if not choice:\n",
    "            continue\n",
    "        m = re.match(r'(\\d)', str(choice))\n",
    "        if not m:\n",
    "            continue\n",
    "        label_int = int(m.group(1))\n",
    "        rows.append({\n",
    "            'task_id': task.get('id'),\n",
    "            'paragraph_text': text,\n",
    "            'label_int': label_int,\n",
    "            'label_text': choice\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Bővített jogi terminus lista\n",
    "LEGAL_TERMS = [\n",
    "    \"szerződés\",\"feltétel\",\"jog\",\"kötelezettség\",\"felelősség\",\"kártérítés\",\"hatály\",\"rendelkezés\",\n",
    "    \"törvény\",\"rendelet\",\"bíróság\",\"per\",\"felmondás\",\"biztosítás\",\"ügyfél\",\"jogosult\",\"kötelezett\",\n",
    "    \"igény\",\"teljesítés\",\"megszűnés\",\"érvényesség\",\"jogviszony\",\"követelés\",\"eljárás\",\"határozat\",\n",
    "    \"jogorvoslat\",\"kikötés\",\"megállapodás\",\"szerződő\",\"felek\",\"szolgáltatás\",\"ellenszolgáltatás\",\n",
    "    \"jogosultság\",\"kötelezettségvállalás\",\"közjegyző\",\"polgári\",\"meghatalmazás\",\"meghatalmazott\"\n",
    "]\n",
    "\n",
    "# Jogi rövidítések\n",
    "LEGAL_ABBREVIATIONS = [\"ptk\",\"kft\",\"zrt\",\"bt\",\"áfa\",\"ászf\",\"gvh\",\"mkeh\",\"mnb\",\"pkkr\"]\n",
    "\n",
    "VOWELS = \"aáeéiíoóöőuúüű\"\n",
    "\n",
    "def count_syllables_hu(word):\n",
    "    count = 0; in_grp = False\n",
    "    for ch in word.lower():\n",
    "        if ch in VOWELS:\n",
    "            if not in_grp:\n",
    "                count += 1; in_grp = True\n",
    "        else:\n",
    "            in_grp = False\n",
    "    return max(1, count)\n",
    "\n",
    "def extract_features(text):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n",
    "    syllable_count = sum(count_syllables_hu(w) for w in words) if words else 0\n",
    "    avg_words_per_sentence = word_count / sentence_count if sentence_count else 0\n",
    "    avg_syllables_per_word = syllable_count / word_count if word_count else 0\n",
    "    \n",
    "    # Alapvető olvashatósági indexek\n",
    "    flesch_score_hu = 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables_per_word if word_count else 0\n",
    "    \n",
    "    # Gunning Fog Index (magyarra adaptálva: komplex szavak = 3+ szótagú szavak)\n",
    "    complex_words = sum(1 for w in words if count_syllables_hu(w) >= 3) if words else 0\n",
    "    complex_word_ratio = complex_words / word_count if word_count else 0\n",
    "    gunning_fog = 0.4 * (avg_words_per_sentence + 100 * complex_word_ratio) if word_count else 0\n",
    "    \n",
    "    # SMOG Index (magyarra adaptálva)\n",
    "    smog_index = 1.0430 * math.sqrt(complex_words * (30 / sentence_count)) + 3.1291 if sentence_count and complex_words else 0\n",
    "    \n",
    "    # Type-Token Ratio (TTR) - szókincs változatosság\n",
    "    unique_words = len(set(words)) if words else 0\n",
    "    ttr = unique_words / word_count if word_count else 0\n",
    "    \n",
    "    # Jogi terminológia\n",
    "    legal_term_ratio = sum(1 for w in words if w in LEGAL_TERMS) / word_count if word_count else 0\n",
    "    legal_abbr_ratio = sum(1 for w in words if w in LEGAL_ABBREVIATIONS) / word_count if word_count else 0\n",
    "    \n",
    "    # Szóhossz jellemzők\n",
    "    long_word_ratio = sum(1 for w in words if len(w) > 12) / word_count if word_count else 0\n",
    "    avg_word_length = sum(len(w) for w in words) / word_count if word_count else 0\n",
    "    \n",
    "    # Szerkezeti jellemzők\n",
    "    comma_count = text.count(',')\n",
    "    semicolon_count = text.count(';')\n",
    "    parenthesis_count = text.count('(') + text.count(')')\n",
    "    comma_ratio = comma_count / word_count if word_count else 0\n",
    "    parenthesis_ratio = parenthesis_count / sentence_count if sentence_count else 0\n",
    "    uppercase_words = sum(1 for w in re.findall(r'\\b[A-ZÁÉÍÓÖŐÚÜŰ]+\\b', text))\n",
    "    uppercase_ratio = uppercase_words / word_count if word_count else 0\n",
    "    \n",
    "    # spaCy elemzés (magyar modellel POS és dependency parsing)\n",
    "    doc = nlp(text)\n",
    "    num_entities = len(doc.ents) if doc.ents else 0\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS) if hasattr(spacy.attrs,'POS') else {}\n",
    "    num_nouns = pos_counts.get(spacy.symbols.NOUN, 0) if hasattr(spacy.symbols,'NOUN') else 0\n",
    "    num_verbs = pos_counts.get(spacy.symbols.VERB, 0) if hasattr(spacy.symbols,'VERB') else 0\n",
    "    num_adjs  = pos_counts.get(spacy.symbols.ADJ, 0) if hasattr(spacy.symbols,'ADJ') else 0\n",
    "    num_advs  = pos_counts.get(spacy.symbols.ADV, 0) if hasattr(spacy.symbols,'ADV') else 0\n",
    "    pos_noun_ratio = num_nouns / word_count if word_count else 0\n",
    "    pos_verb_ratio = num_verbs / word_count if word_count else 0\n",
    "    pos_adj_ratio  = num_adjs  / word_count if word_count else 0\n",
    "    pos_adv_ratio  = num_advs  / word_count if word_count else 0\n",
    "    \n",
    "    # Dependency parsing mélység\n",
    "    depths = []\n",
    "    for token in doc:\n",
    "        d=0; cur=token\n",
    "        while cur.head != cur and d < 100:\n",
    "            d += 1; cur = cur.head\n",
    "        depths.append(d)\n",
    "    avg_dep_depth = float(np.mean(depths)) if depths else 0\n",
    "    max_dep_depth = float(max(depths)) if depths else 0\n",
    "    \n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'syllable_count': syllable_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'flesch_score_hu': flesch_score_hu,\n",
    "        'gunning_fog': gunning_fog,\n",
    "        'smog_index': smog_index,\n",
    "        'ttr': ttr,\n",
    "        'complex_word_ratio': complex_word_ratio,\n",
    "        'legal_term_ratio': legal_term_ratio,\n",
    "        'legal_abbr_ratio': legal_abbr_ratio,\n",
    "        'long_word_ratio': long_word_ratio,\n",
    "        'comma_ratio': comma_ratio,\n",
    "        'parenthesis_ratio': parenthesis_ratio,\n",
    "        'uppercase_ratio': uppercase_ratio,\n",
    "        'num_entities': num_entities,\n",
    "        'pos_noun_ratio': pos_noun_ratio,\n",
    "        'pos_verb_ratio': pos_verb_ratio,\n",
    "        'pos_adj_ratio': pos_adj_ratio,\n",
    "        'pos_adv_ratio': pos_adv_ratio,\n",
    "        'avg_dep_depth': avg_dep_depth,\n",
    "        'max_dep_depth': max_dep_depth\n",
    "    }\n",
    "\n",
    "# Annotációk betöltése az új függvénnyel\n",
    "df_labels = load_annotation_json(ANNOTATION_FILE)\n",
    "if not df_labels.empty:\n",
    "    print(f\"Betöltött címkék száma: {len(df_labels)}\")\n",
    "    print(\"Címkék eloszlása:\")\n",
    "    print(df_labels['label_int'].value_counts().sort_index())\n",
    "    print(\"\\nAdatminta:\")\n",
    "    print(df_labels.head())\n",
    "else:\n",
    "    print(\"Nem sikerült adatokat betölteni. A további feature / modell lépések kihagyva amíg nincs adat.\")\n",
    "\n",
    "# Feature kinyerés csak ha van adat\n",
    "if not df_labels.empty:\n",
    "    feature_rows = [extract_features(t) for t in tqdm(df_labels['paragraph_text'], desc='Feature extract')] \n",
    "    df_feat = pd.DataFrame(feature_rows)\n",
    "    df_processed = pd.concat([df_labels.reset_index(drop=True), df_feat], axis=1)\n",
    "    print('Feature oszlopok:', list(df_feat.columns)[:8], '...')\n",
    "    print(df_processed.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb40083",
   "metadata": {},
   "source": [
    "# 4. Alapmodell (Baseline) - Ordinális Regresszió\n",
    "\n",
    "Egy egyszerű, de hatékony alapmodellt tanítunk a kinyert jellemzőkön a `mord` csomag segítségével."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba7c616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LogisticAT MAE: 1.1250\n"
     ]
    }
   ],
   "source": [
    "# 5. Baseline Ordinal Regression (LogisticAT) haladó feature-ökkel\n",
    "if 'df_processed' in globals() and not df_processed.empty:\n",
    "    # Frissítve, hogy az összes új feature-t használja\n",
    "    baseline_feature_cols = [\n",
    "        'char_count', 'word_count', 'sentence_count', 'syllable_count', \n",
    "        'avg_word_length', 'flesch_score_hu', 'gunning_fog', 'smog_index', 'ttr', \n",
    "        'complex_word_ratio', 'legal_term_ratio', 'legal_abbr_ratio', \n",
    "        'long_word_ratio', 'comma_ratio', 'parenthesis_ratio', 'uppercase_ratio', \n",
    "        'num_entities', 'pos_noun_ratio', 'pos_verb_ratio', 'pos_adj_ratio', \n",
    "        'pos_adv_ratio', 'avg_dep_depth', 'max_dep_depth'\n",
    "    ]\n",
    "    missing = [c for c in baseline_feature_cols if c not in df_processed.columns]\n",
    "    if missing:\n",
    "        print('Hiányzó feature oszlopok, baseline kihagyva:', missing)\n",
    "    else:\n",
    "        Xb = df_processed[baseline_feature_cols].values\n",
    "        yb = df_processed['label_int'].values\n",
    "        X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(Xb, yb, test_size=VAL_SIZE, random_state=SEED, stratify=yb)\n",
    "        scaler_b = StandardScaler()\n",
    "        X_train_b = scaler_b.fit_transform(X_train_b)\n",
    "        X_val_b = scaler_b.transform(X_val_b)\n",
    "        baseline_model = LogisticAT(alpha=0.5)\n",
    "        baseline_model.fit(X_train_b, y_train_b)\n",
    "        preds_b = baseline_model.predict(X_val_b)\n",
    "        mae_b = mean_absolute_error(y_val_b, preds_b)\n",
    "        print(f'Baseline LogisticAT MAE: {mae_b:.4f}')\n",
    "else:\n",
    "    print('Baseline nem futtatható: df_processed hiányzik vagy üres.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f91436",
   "metadata": {},
   "source": [
    "# 5. Transformer Modell (CORAL)\n",
    "\n",
    "Egy neurális háló alapú modellt definiálunk, amely egy előtanított transzformert (pl. huBERT) használ, és egy CORAL (Cumulative Ordinal Ranking and Regression) kimeneti réteggel egészíti ki az ordinális klasszifikációhoz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e7fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer + CORAL modell betöltve. Extra feature dim: 23\n"
     ]
    }
   ],
   "source": [
    "# 6. Adatkészlet és DataLoader a Transformerhez\n",
    "\n",
    "# Definiáljuk az összes új feature oszlopot\n",
    "advanced_feature_cols = [\n",
    "    'char_count', 'word_count', 'sentence_count', 'syllable_count', \n",
    "    'avg_word_length', 'flesch_score_hu', 'gunning_fog', 'smog_index', 'ttr', \n",
    "    'complex_word_ratio', 'legal_term_ratio', 'legal_abbr_ratio', \n",
    "    'long_word_ratio', 'comma_ratio', 'parenthesis_ratio', 'uppercase_ratio', \n",
    "    'num_entities', 'pos_noun_ratio', 'pos_verb_ratio', 'pos_adj_ratio', \n",
    "    'pos_adv_ratio', 'avg_dep_depth', 'max_dep_depth'\n",
    "]\n",
    "\n",
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, feature_cols, feature_stats=None):\n",
    "        self.texts = df['paragraph_text'].values\n",
    "        self.labels = df['label_int'].values\n",
    "        self.features = df[feature_cols].values if feature_cols else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_stats = feature_stats\n",
    "\n",
    "        if self.features is not None and self.feature_stats:\n",
    "            # Normalizálás a tanító adathalmaz statisztikái alapján\n",
    "            means = np.array([self.feature_stats[col][0] for col in feature_cols])\n",
    "            stds = np.array([self.feature_stats[col][1] for col in feature_cols])\n",
    "            self.features = (self.features - means) / stds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.features is not None:\n",
    "            item['features'] = torch.tensor(self.features[idx], dtype=torch.float)\n",
    "        else:\n",
    "            item['features'] = torch.empty(0, dtype=torch.float)\n",
    "            \n",
    "        return item\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size, feature_cols, feature_stats=None):\n",
    "    ds = LegalDataset(\n",
    "        df=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        feature_cols=feature_cols,\n",
    "        feature_stats=feature_stats\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# --- CORAL Modell definíciója (feature integráció + expected value támogatás) ---\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "class CoralHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, extra_feat_dim=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.use_extra = extra_feat_dim > 0\n",
    "        in_dim = hidden_size + extra_feat_dim if self.use_extra else hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(in_dim, num_classes - 1)\n",
    "    def forward(self, cls_hidden, extra_feats=None):\n",
    "        if self.use_extra and extra_feats is not None:\n",
    "            x = torch.cat([cls_hidden, extra_feats], dim=1)\n",
    "        else:\n",
    "            x = cls_hidden\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "def coral_probs_to_label_argmax(probs):\n",
    "    batch_size = probs.size(0)\n",
    "    ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "    zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "    p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "    p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "    return torch.argmax(p_exact, dim=1) + 1\n",
    "\n",
    "def coral_probs_to_label_expected(probs):\n",
    "    batch_size = probs.size(0)\n",
    "    ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "    zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "    p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "    p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "    labels = torch.arange(1, probs.size(1)+2, device=probs.device).float()  # 1..K\n",
    "    exp_val = torch.sum(p_exact * labels, dim=1)\n",
    "    return torch.clamp(torch.round(exp_val), 1, probs.size(1)+1).long()\n",
    "\n",
    "class CoralModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=5, extra_feat_dim=0):\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.base.config.hidden_size\n",
    "        self.head = CoralHead(hidden_size, num_classes, extra_feat_dim=extra_feat_dim, dropout=0.3)\n",
    "    def forward(self, input_ids, attention_mask, extra_feats=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        probs = self.head(cls_output, extra_feats)\n",
    "        return probs\n",
    "\n",
    "def build_pos_weights(labels, num_classes):\n",
    "    # labels 1..K; for each threshold k (1..K-1) compute pos = labels>k\n",
    "    weights = []\n",
    "    labels_t = torch.tensor(labels)\n",
    "    for k in range(1, num_classes):\n",
    "        pos = (labels_t > k).sum().item()\n",
    "        neg = (labels_t <= k).sum().item()\n",
    "        if pos == 0: pos = 1\n",
    "        w_pos = neg / pos  # emphasize minority positives\n",
    "        weights.append(w_pos)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "def coral_loss_weighted(probs, labels, pos_weights):\n",
    "    # probs shape (B, K-1); labels 1..K\n",
    "    K = probs.size(1) + 1\n",
    "    targets = []\n",
    "    for k in range(1, K):\n",
    "        target_k = (labels > k).float().unsqueeze(1)\n",
    "        targets.append(target_k)\n",
    "    target_tensor = torch.cat(targets, dim=1)  # (B, K-1)\n",
    "    # Weighted BCE manually\n",
    "    # pos_weight for positive target; negative weight = 1\n",
    "    pw = pos_weights.to(probs.device).unsqueeze(0)  # (1,K-1)\n",
    "    loss_pos = -pw * target_tensor * torch.log(probs + 1e-8)\n",
    "    loss_neg = -(1 - target_tensor) * torch.log(1 - probs + 1e-8)\n",
    "    loss = (loss_pos + loss_neg).mean()\n",
    "    return loss\n",
    "\n",
    "# Tokenizer + modell példányosítás extra feature dim alapján\n",
    "extra_dim = len(advanced_feature_cols)\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "coral_model = CoralModel(MODEL_NAME, num_classes=NUM_CLASSES, extra_feat_dim=extra_dim)\n",
    "print('Transformer + CORAL modell betöltve. Extra feature dim:', extra_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6927ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Train size=95 Val size=24\n",
      "Fold 1 Epoch 1: TrainLoss=0.6303 ValLoss=0.5237 ValMAE=1.1667\n",
      "Fold 1 Epoch 1: TrainLoss=0.6303 ValLoss=0.5237 ValMAE=1.1667\n",
      "Fold 1 Epoch 2: TrainLoss=0.5307 ValLoss=0.5585 ValMAE=2.0833\n",
      "Fold 1 Epoch 2: TrainLoss=0.5307 ValLoss=0.5585 ValMAE=2.0833\n",
      "Fold 1 Epoch 3: TrainLoss=0.5431 ValLoss=0.5324 ValMAE=0.9167\n",
      "Fold 1 Epoch 3: TrainLoss=0.5431 ValLoss=0.5324 ValMAE=0.9167\n",
      "Fold 1 Epoch 4: TrainLoss=0.5094 ValLoss=0.5103 ValMAE=0.9583\n",
      "Fold 1 Epoch 4: TrainLoss=0.5094 ValLoss=0.5103 ValMAE=0.9583\n",
      "Fold 1 Epoch 5: TrainLoss=0.4981 ValLoss=0.5067 ValMAE=0.7917\n",
      "Fold 1 Epoch 5: TrainLoss=0.4981 ValLoss=0.5067 ValMAE=0.7917\n",
      "Fold 1 Epoch 6: TrainLoss=0.4924 ValLoss=0.5075 ValMAE=0.7917\n",
      "Fold 1 best MAE: 0.7917\n",
      "\n",
      "[Fold 2/5] Train size=95 Val size=24\n",
      "Fold 1 Epoch 6: TrainLoss=0.4924 ValLoss=0.5075 ValMAE=0.7917\n",
      "Fold 1 best MAE: 0.7917\n",
      "\n",
      "[Fold 2/5] Train size=95 Val size=24\n",
      "Fold 2 Epoch 1: TrainLoss=0.6491 ValLoss=0.5107 ValMAE=0.9583\n",
      "Fold 2 Epoch 1: TrainLoss=0.6491 ValLoss=0.5107 ValMAE=0.9583\n",
      "Fold 2 Epoch 2: TrainLoss=0.5713 ValLoss=0.5145 ValMAE=1.0000\n",
      "Fold 2 Epoch 2: TrainLoss=0.5713 ValLoss=0.5145 ValMAE=1.0000\n",
      "Fold 2 Epoch 3: TrainLoss=0.5379 ValLoss=0.5177 ValMAE=1.0000\n",
      "Early stopping fold 2\n",
      "Fold 2 best MAE: 0.9583\n",
      "\n",
      "[Fold 3/5] Train size=95 Val size=24\n",
      "Fold 2 Epoch 3: TrainLoss=0.5379 ValLoss=0.5177 ValMAE=1.0000\n",
      "Early stopping fold 2\n",
      "Fold 2 best MAE: 0.9583\n",
      "\n",
      "[Fold 3/5] Train size=95 Val size=24\n",
      "Fold 3 Epoch 1: TrainLoss=0.6870 ValLoss=0.5663 ValMAE=1.2917\n",
      "Fold 3 Epoch 1: TrainLoss=0.6870 ValLoss=0.5663 ValMAE=1.2917\n",
      "Fold 3 Epoch 2: TrainLoss=0.5413 ValLoss=0.5390 ValMAE=1.0000\n",
      "Fold 3 Epoch 2: TrainLoss=0.5413 ValLoss=0.5390 ValMAE=1.0000\n",
      "Fold 3 Epoch 3: TrainLoss=0.5286 ValLoss=0.5267 ValMAE=1.0000\n",
      "Fold 3 Epoch 3: TrainLoss=0.5286 ValLoss=0.5267 ValMAE=1.0000\n",
      "Fold 3 Epoch 4: TrainLoss=0.5253 ValLoss=0.5560 ValMAE=1.0000\n",
      "Early stopping fold 3\n",
      "Fold 3 best MAE: 1.0000\n",
      "\n",
      "[Fold 4/5] Train size=95 Val size=24\n",
      "Fold 3 Epoch 4: TrainLoss=0.5253 ValLoss=0.5560 ValMAE=1.0000\n",
      "Early stopping fold 3\n",
      "Fold 3 best MAE: 1.0000\n",
      "\n",
      "[Fold 4/5] Train size=95 Val size=24\n",
      "Fold 4 Epoch 1: TrainLoss=0.6649 ValLoss=0.5208 ValMAE=0.7917\n",
      "Fold 4 Epoch 1: TrainLoss=0.6649 ValLoss=0.5208 ValMAE=0.7917\n",
      "Fold 4 Epoch 2: TrainLoss=0.5833 ValLoss=0.5281 ValMAE=0.9583\n",
      "Fold 4 Epoch 2: TrainLoss=0.5833 ValLoss=0.5281 ValMAE=0.9583\n",
      "Fold 4 Epoch 3: TrainLoss=0.5366 ValLoss=0.5366 ValMAE=0.9583\n",
      "Early stopping fold 4\n",
      "Fold 4 best MAE: 0.7917\n",
      "\n",
      "[Fold 5/5] Train size=96 Val size=23\n",
      "Fold 4 Epoch 3: TrainLoss=0.5366 ValLoss=0.5366 ValMAE=0.9583\n",
      "Early stopping fold 4\n",
      "Fold 4 best MAE: 0.7917\n",
      "\n",
      "[Fold 5/5] Train size=96 Val size=23\n",
      "Fold 5 Epoch 1: TrainLoss=0.7239 ValLoss=0.5271 ValMAE=1.2609\n",
      "Fold 5 Epoch 1: TrainLoss=0.7239 ValLoss=0.5271 ValMAE=1.2609\n",
      "Fold 5 Epoch 2: TrainLoss=0.5358 ValLoss=0.4672 ValMAE=0.8696\n",
      "Fold 5 Epoch 2: TrainLoss=0.5358 ValLoss=0.4672 ValMAE=0.8696\n",
      "Fold 5 Epoch 3: TrainLoss=0.5183 ValLoss=0.4665 ValMAE=0.8261\n",
      "Fold 5 Epoch 3: TrainLoss=0.5183 ValLoss=0.4665 ValMAE=0.8261\n",
      "Fold 5 Epoch 4: TrainLoss=0.5223 ValLoss=0.4580 ValMAE=0.7391\n",
      "Fold 5 Epoch 4: TrainLoss=0.5223 ValLoss=0.4580 ValMAE=0.7391\n",
      "Fold 5 Epoch 5: TrainLoss=0.5321 ValLoss=0.4571 ValMAE=0.7826\n",
      "Fold 5 Epoch 5: TrainLoss=0.5321 ValLoss=0.4571 ValMAE=0.7826\n",
      "Fold 5 Epoch 6: TrainLoss=0.5031 ValLoss=0.4538 ValMAE=0.7391\n",
      "Early stopping fold 5\n",
      "Fold 5 best MAE: 0.7391\n",
      "\n",
      "K-Fold MAE-k: [0.7916666666666666, 0.9583333333333334, 1.0, 0.7916666666666666, 0.7391304347826086]\n",
      "Átlagos MAE (fold átlag): 0.856159420289855\n",
      "Fold 5 Epoch 6: TrainLoss=0.5031 ValLoss=0.4538 ValMAE=0.7391\n",
      "Early stopping fold 5\n",
      "Fold 5 best MAE: 0.7391\n",
      "\n",
      "K-Fold MAE-k: [0.7916666666666666, 0.9583333333333334, 1.0, 0.7916666666666666, 0.7391304347826086]\n",
      "Átlagos MAE (fold átlag): 0.856159420289855\n"
     ]
    }
   ],
   "source": [
    "# 7. 5-Fold Stratified Cross-Validation Ensemble\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "KFOLDS = 5\n",
    "EPOCHS_KF = 6\n",
    "PATIENCE_KF = 2\n",
    "LEARNING_RATE_KF = 2e-5\n",
    "\n",
    "if 'df_processed' in globals() and not df_processed.empty:\n",
    "    skf = StratifiedKFold(n_splits=KFOLDS, shuffle=True, random_state=SEED)\n",
    "    labels_all = df_processed['label_int'].values\n",
    "    fold_results = []\n",
    "    ensemble_exact_probs = []  # store per-fold val exact class prob for ensemble\n",
    "\n",
    "    # Helper: simple CORAL loss\n",
    "    def coral_loss_simple(probs, labels):\n",
    "        K = probs.size(1) + 1\n",
    "        targets = []\n",
    "        for k in range(1, K):\n",
    "            target_k = (labels > k).float().unsqueeze(1)\n",
    "            targets.append(target_k)\n",
    "        target_tensor = torch.cat(targets, dim=1)\n",
    "        return nn.BCELoss()(probs, target_tensor)\n",
    "\n",
    "    # Function to get exact class probabilities from cumulative probs\n",
    "    def cumulative_to_exact(probs):\n",
    "        batch_size = probs.size(0)\n",
    "        ones = torch.ones(batch_size, 1, device=probs.device)\n",
    "        zeros = torch.zeros(batch_size, 1, device=probs.device)\n",
    "        p_greater_than = torch.cat([ones, probs, zeros], dim=1)\n",
    "        p_exact = p_greater_than[:, :-1] - p_greater_than[:, 1:]\n",
    "        return p_exact  # shape (B, K)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_processed, labels_all), start=1):\n",
    "        print(f'\\n[Fold {fold}/{KFOLDS}] Train size={len(train_idx)} Val size={len(val_idx)}')\n",
    "        df_train_f = df_processed.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val_f = df_processed.iloc[val_idx].reset_index(drop=True)\n",
    "        # Stats per fold\n",
    "        feature_stats_f = {c: (df_train_f[c].mean(), df_train_f[c].std() if df_train_f[c].std() > 0 else 1.0) for c in advanced_feature_cols}\n",
    "\n",
    "        train_loader_f = create_data_loader(df_train_f, tokenizer, MAX_LEN, BATCH_SIZE, advanced_feature_cols, feature_stats_f)\n",
    "        val_loader_f = create_data_loader(df_val_f, tokenizer, MAX_LEN, BATCH_SIZE, advanced_feature_cols, feature_stats_f)\n",
    "\n",
    "        # Fresh model per fold\n",
    "        model_f = CoralModel(MODEL_NAME, num_classes=NUM_CLASSES, extra_feat_dim=len(advanced_feature_cols)).to(DEVICE)\n",
    "        optimizer_f = torch.optim.AdamW(model_f.parameters(), lr=LEARNING_RATE_KF)\n",
    "        total_steps_f = len(train_loader_f) * EPOCHS_KF\n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "        scheduler_f = get_linear_schedule_with_warmup(optimizer_f, num_warmup_steps=0, num_training_steps=total_steps_f)\n",
    "\n",
    "        best_mae_f = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(EPOCHS_KF):\n",
    "            model_f.train(); train_losses=[]\n",
    "            for batch in train_loader_f:\n",
    "                ids = batch['input_ids'].to(DEVICE)\n",
    "                mask = batch['attention_mask'].to(DEVICE)\n",
    "                feats = batch['features'].to(DEVICE) if batch['features'] is not None else None\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                probs = model_f(ids, mask, extra_feats=feats)\n",
    "                loss = coral_loss_simple(probs, labels)\n",
    "                train_losses.append(loss.item())\n",
    "                loss.backward(); nn.utils.clip_grad_norm_(model_f.parameters(),1.0)\n",
    "                optimizer_f.step(); scheduler_f.step(); optimizer_f.zero_grad()\n",
    "            # Validation\n",
    "            model_f.eval(); all_lab=[]; all_pred=[]; val_losses=[]; val_exact_prob_collect=[]\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader_f:\n",
    "                    ids = batch['input_ids'].to(DEVICE)\n",
    "                    mask = batch['attention_mask'].to(DEVICE)\n",
    "                    feats = batch['features'].to(DEVICE) if batch['features'] is not None else None\n",
    "                    labels = batch['labels'].to(DEVICE)\n",
    "                    probs = model_f(ids, mask, extra_feats=feats)\n",
    "                    loss = coral_loss_simple(probs, labels)\n",
    "                    val_losses.append(loss.item())\n",
    "                    p_exact = cumulative_to_exact(probs)\n",
    "                    preds = torch.argmax(p_exact, dim=1) + 1\n",
    "                    all_lab.extend(labels.cpu().numpy())\n",
    "                    all_pred.extend(preds.cpu().numpy())\n",
    "                    val_exact_prob_collect.append(p_exact.cpu())\n",
    "            mae_f = mean_absolute_error(all_lab, all_pred)\n",
    "            print(f'Fold {fold} Epoch {epoch+1}: TrainLoss={np.mean(train_losses):.4f} ValLoss={np.mean(val_losses):.4f} ValMAE={mae_f:.4f}')\n",
    "            if mae_f < best_mae_f:\n",
    "                best_mae_f = mae_f\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model_f.state_dict(), f'coral_fold{fold}.bin')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= PATIENCE_KF:\n",
    "                    print('Early stopping fold', fold)\n",
    "                    break\n",
    "        print(f'Fold {fold} best MAE: {best_mae_f:.4f}')\n",
    "        fold_results.append(best_mae_f)\n",
    "        # Store ensemble probabilities from best epoch (we used last collected p_exact list)\n",
    "        ensemble_exact_probs.append(torch.cat(val_exact_prob_collect, dim=0))\n",
    "    mean_mae = np.mean(fold_results)\n",
    "    print('\\nK-Fold MAE-k:', fold_results)\n",
    "    print('Átlagos MAE (fold átlag):', mean_mae)\n",
    "    # Ensemble across folds on their own validation splits is not directly comparable; we can just report mean.\n",
    "    # (Optionally could retrain on full data with averaged thresholds.)\n",
    "else:\n",
    "    print('Nincs adat a K-fold futtatáshoz.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
